{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihwd6wbQeFdk"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idyl7Hltwev8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ1xdhF9epQ5"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s3GNNh3yKkL"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://test.pypi.org/simple/ supervision==0.3.0\n",
        "!pip install -q transformers\n",
        "!pip install -q pytorch-lightning\n",
        "!pip install -q roboflow\n",
        "!pip install -q timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkBxWG_koPsg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "import roboflow\n",
        "import supervision\n",
        "import transformers\n",
        "import pytorch_lightning\n",
        "\n",
        "print(\n",
        "    \"roboflow:\", roboflow.__version__,\n",
        "    \"; supervision:\", supervision.__version__,\n",
        "    \"; transformers:\", transformers.__version__,\n",
        "    \"; pytorch_lightning:\", pytorch_lightning.__version__\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe6r7u1bffJ1"
      },
      "source": [
        "## Inference with pre-trained COCO model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hddhx7oPgHXJ"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ij8_XssfkQg"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "!wget https://media.roboflow.com/notebooks/examples/dog.jpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4gxmpNJgRbH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "IMAGE_NAME = \"dog.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, IMAGE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHQTqHsQgsCZ"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w70Rgx-ydq2L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
        "\n",
        "\n",
        "# settings\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = 'facebook/detr-resnet-50'\n",
        "CONFIDENCE_TRESHOLD = 0.5\n",
        "IOU_TRESHOLD = 0.8\n",
        "\n",
        "image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
        "model = DetrForObjectDetection.from_pretrained(CHECKPOINT)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z95T5a3ciF6b"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToSzLwjHiMP_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import supervision as sv\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    image = cv2.imread(IMAGE_PATH)\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs,\n",
        "        threshold=CONFIDENCE_TRESHOLD,\n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_transformers(transformers_results=results)\n",
        "\n",
        "labels = [\n",
        "    f\"{model.config.id2label[class_id]} {confidence:0.2f}\"\n",
        "    for _, confidence, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nrM8p9QuhZC"
      },
      "source": [
        "**NOTE:** It seems that we have a lot of excess detections. Let's try to filter them out using non-max suppression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehle9xKWzhTP"
      },
      "source": [
        "## Inference + NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRD7HQCZzRUz"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import supervision as sv\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    image = cv2.imread(IMAGE_PATH)\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs,\n",
        "        threshold=CONFIDENCE_TRESHOLD,\n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=IOU_TRESHOLD)\n",
        "\n",
        "labels = [\n",
        "    f\"{model.config.id2label[class_id]} {confidence:0.2f}\"\n",
        "    for _, confidence, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut0S6jbzfYFW"
      },
      "source": [
        "## Download custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hkRhqPEC1tL",
        "outputId": "d66b6ac7-529c-444c-efc5-ee0e2c1ba9eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter ROBOFLOW_API_KEY secret value: ··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "ROBOFLOW_API_KEY = getpass('Enter ROBOFLOW_API_KEY secret value: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_APffCi2eY9"
      },
      "outputs": [],
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"7qgPIIojmf4efk4WYNMV\")\n",
        "project = rf.workspace(\"detr-trial\").project(\"detr-trial-data-330\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"coco\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXKyMYI4vTv9"
      },
      "source": [
        "**NOTE:** We can find out where our dataset was saved using the `dataset.location` property."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pH8suSKrmz5_",
        "outputId": "ef2539f9-9d59-4ad4-f098-ab582bfe03eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/datasets/detr-trial-data-330-1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataset.location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQFUTecIXDUs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0gIu1AJrAeg"
      },
      "source": [
        "## Create COCO data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk6sRB0lueHY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torchvision\n",
        "\n",
        "\n",
        "# settings\n",
        "ANNOTATION_FILE_NAME = \"_annotations.coco.json\"\n",
        "TRAIN_DIRECTORY = os.path.join(dataset.location, \"train\")\n",
        "VAL_DIRECTORY = os.path.join(dataset.location, \"valid\")\n",
        "TEST_DIRECTORY = os.path.join(dataset.location, \"test\")\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_directory_path: str,\n",
        "        image_processor,\n",
        "        train: bool = True\n",
        "    ):\n",
        "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
        "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
        "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
        "        target = encoding[\"labels\"][0]\n",
        "\n",
        "        return pixel_values, target\n",
        "\n",
        "\n",
        "TRAIN_DATASET = CocoDetection(\n",
        "    image_directory_path=TRAIN_DIRECTORY,\n",
        "    image_processor=image_processor,\n",
        "    train=True)\n",
        "VAL_DATASET = CocoDetection(\n",
        "    image_directory_path=VAL_DIRECTORY,\n",
        "    image_processor=image_processor,\n",
        "    train=False)\n",
        "TEST_DATASET = CocoDetection(\n",
        "    image_directory_path=TEST_DIRECTORY,\n",
        "    image_processor=image_processor,\n",
        "    train=False)\n",
        "\n",
        "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
        "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
        "print(\"Number of test examples:\", len(TEST_DATASET))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeT9rfsMwj5O"
      },
      "source": [
        "## Visualize data entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85pfhh5FwoVm"
      },
      "source": [
        "**NOTE:** Feel free to reload this cell multiple times. Notebook should display different train set image each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoRf4rTewQKc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# select random image\n",
        "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons\n",
        "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "\n",
        "# we will use id2label function for training\n",
        "categories = TRAIN_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "\n",
        "labels = [\n",
        "    f\"{id2label[class_id]}\"\n",
        "    for _, _, class_id, _\n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(image, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OPAnqJ-wxod"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # DETR authors employ various image sizes during training, making it not possible\n",
        "    # to directly batch together images. Hence they pad the images to the biggest\n",
        "    # resolution in a given batch, and create a corresponding binary pixel_mask\n",
        "    # which indicates which pixels are real/which are padding\n",
        "    pixel_values = [item[0] for item in batch]\n",
        "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
        "    labels = [item[1] for item in batch]\n",
        "    return {\n",
        "        'pixel_values': encoding['pixel_values'],\n",
        "        'pixel_mask': encoding['pixel_mask'],\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=5, shuffle=True)\n",
        "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=5)\n",
        "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vVPm-sO4C9W"
      },
      "source": [
        "## Train model with PyTorch Lightning\n",
        "\n",
        "**NOTE:** Here we define a regular PyTorch dataset. Each item of the dataset is an image and corresponding annotations. Torchvision already provides a `CocoDetection` dataset, which we can use. We only add a feature extractor (`DetrImageProcessor`) to resize + normalize the images, and to turn the annotations (which are in COCO format) in the format that DETR expects. It will also resize the annotations accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzOJhEPa39dZ"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from transformers import DetrForObjectDetection\n",
        "import torch\n",
        "\n",
        "\n",
        "class Detr(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, lr, lr_backbone, weight_decay):\n",
        "        super().__init__()\n",
        "        self.model = DetrForObjectDetection.from_pretrained(\n",
        "            pretrained_model_name_or_path=CHECKPOINT,\n",
        "            num_labels=len(id2label),\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        self.lr = lr\n",
        "        self.lr_backbone = lr_backbone\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def forward(self, pixel_values, pixel_mask):\n",
        "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    def common_step(self, batch, batch_idx):\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        pixel_mask = batch[\"pixel_mask\"]\n",
        "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss_dict = outputs.loss_dict\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
        "        # logs metrics for each training_step, and the average across the epoch\n",
        "        self.log(\"training_loss\", loss)\n",
        "        for k,v in loss_dict.items():\n",
        "            self.log(\"train_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
        "        self.log(\"validation/loss\", loss)\n",
        "        for k, v in loss_dict.items():\n",
        "            self.log(\"validation_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # DETR authors decided to use different learning rate for backbone\n",
        "        # you can learn more about it here:\n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
        "        param_dicts = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "                \"lr\": self.lr_backbone,\n",
        "            },\n",
        "        ]\n",
        "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return TRAIN_DATALOADER\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return VAL_DATALOADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke7XS8_C1UUM"
      },
      "source": [
        "**NOTE:** Let's start `tensorboard`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXZCQ6j-F9kr"
      },
      "outputs": [],
      "source": [
        "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
        "\n",
        "batch = next(iter(TRAIN_DATALOADER))\n",
        "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSPRLP6aGOpV",
        "outputId": "9317d4c9-6b7b-40a1-eef9-625dea826a5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 100, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "outputs.logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGKC5yOyHAIJ"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-cJEJvAGjTB"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "%cd {HOME}\n",
        "\n",
        "# settings\n",
        "MAX_EPOCHS = 50\n",
        "\n",
        "# pytorch_lightning < 2.0.0\n",
        "# trainer = Trainer(gpus=1, max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
        "\n",
        "# pytorch_lightning >= 2.0.0\n",
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=1)\n",
        "\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trying to get stuff from tensor board\n",
        "\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to your TensorBoard logs directory\n",
        "log_dir = \"/content/lightning_logs/version_0\"\n",
        "\n",
        "# Load the event file\n",
        "ea = event_accumulator.EventAccumulator(log_dir)\n",
        "ea.Reload()\n",
        "\n",
        "# Get scalar tags\n",
        "scalar_tags = [\n",
        "    \"epoch\",\n",
        "    \"train_cardinality_error\",\n",
        "    \"train_loss_bbox\",\n",
        "    \"train_loss_ce\",\n",
        "    \"train_loss_giou\",\n",
        "    \"training_loss\",\n",
        "    \"validation\",\n",
        "    \"validation_cardinality_error\",\n",
        "    \"validation_loss_bbox\",\n",
        "    \"validation_loss_ce\",\n",
        "    \"validation_loss_giou\"\n",
        "]\n",
        "\n",
        "# Create export folder\n",
        "export_dir = \"/content/tensorboard_exports\"\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "# Export each scalar tag to CSV\n",
        "for tag in scalar_tags:\n",
        "    if tag in ea.Tags()[\"scalars\"]:\n",
        "        events = ea.Scalars(tag)\n",
        "        df = pd.DataFrame([(e.step, e.value) for e in events], columns=[\"step\", \"value\"])\n",
        "        filename = os.path.join(export_dir, f\"{tag.replace('/', '_')}.csv\")\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Exported: {filename}\")\n",
        "    else:\n",
        "        print(f\"Tag '{tag}' not found.\")"
      ],
      "metadata": {
        "id": "bBz619LUyBci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Path to your Drive folder (you can change 'detr_exports' to whatever you like)\n",
        "drive_export_path = \"/content/drive/MyDrive/detr_exports\"\n",
        "os.makedirs(drive_export_path, exist_ok=True)\n",
        "\n",
        "# Copy CSVs from tensorboard_exports to Drive\n",
        "local_export_path = \"/content/tensorboard_exports\"\n",
        "\n",
        "for filename in os.listdir(local_export_path):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        shutil.copy(os.path.join(local_export_path, filename), drive_export_path)\n",
        "\n",
        "print(\"✅ All CSVs copied to Google Drive:\", drive_export_path)\n"
      ],
      "metadata": {
        "id": "jTtzlchbzcjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrewgWdGQULA"
      },
      "source": [
        "## Inference on test dataset\n",
        "\n",
        "Let's visualize the predictions of DETR on the first image of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"detr_model.pth\")\n"
      ],
      "metadata": {
        "id": "WyCiwDa9deMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')\n"
      ],
      "metadata": {
        "id": "JG_zaGVafX75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6YfGgeATvbR"
      },
      "outputs": [],
      "source": [
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbzTzHJW22up"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# utils\n",
        "categories = TEST_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "\n",
        "# select random image\n",
        "image_ids = TEST_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons\n",
        "image = TEST_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TEST_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TEST_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "labels = [f\"{id2label[class_id]}\" for _, _, class_id, _ in detections]\n",
        "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "print('ground truth')\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))\n",
        "\n",
        "# inference\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs,\n",
        "        threshold=CONFIDENCE_TRESHOLD,\n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=0.5)\n",
        "labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
        "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "print('detections')\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y695PeIASOjF"
      },
      "source": [
        "## Evaluation on test dataset\n",
        "\n",
        "Finally, we evaluate the model on the `TEST_DATASET`. For this we make use of the `CocoEvaluator` class available in a tiny [PyPi package](https://github.com/NielsRogge/coco-eval) made by [Niels Rogge](https://github.com/NielsRogge) . This class is entirely based on the [original evaluator](https://github.com/facebookresearch/detr/blob/main/datasets/coco_eval.py) class used by the DETR authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiEaI_vpSz2i"
      },
      "outputs": [],
      "source": [
        "!pip install -q coco_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQx9pgaHVScS"
      },
      "outputs": [],
      "source": [
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "def prepare_for_coco_detection(predictions):\n",
        "    coco_results = []\n",
        "    for original_id, prediction in predictions.items():\n",
        "        if len(prediction) == 0:\n",
        "            continue\n",
        "\n",
        "        boxes = prediction[\"boxes\"]\n",
        "        boxes = convert_to_xywh(boxes).tolist()\n",
        "        scores = prediction[\"scores\"].tolist()\n",
        "        labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "        coco_results.extend(\n",
        "            [\n",
        "                {\n",
        "                    \"image_id\": original_id,\n",
        "                    \"category_id\": labels[k],\n",
        "                    \"bbox\": box,\n",
        "                    \"score\": scores[k],\n",
        "                }\n",
        "                for k, box in enumerate(boxes)\n",
        "            ]\n",
        "        )\n",
        "    return coco_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts6bI87iTZzY"
      },
      "outputs": [],
      "source": [
        "from coco_eval import CocoEvaluator\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "evaluator = CocoEvaluator(coco_gt=TEST_DATASET.coco, iou_types=[\"bbox\"])\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "for idx, batch in enumerate(tqdm(TEST_DATALOADER)):\n",
        "    pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
        "    pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
        "    labels = [{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
        "    results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes)\n",
        "\n",
        "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
        "    predictions = prepare_for_coco_detection(predictions)\n",
        "    evaluator.update(predictions)\n",
        "\n",
        "evaluator.synchronize_between_processes()\n",
        "evaluator.accumulate()\n",
        "evaluator.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnDuaKqGW6Bl"
      },
      "source": [
        "## Save and load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FPFeXx2XgcW"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = os.path.join(HOME, 'custom-model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYIAdWm9W3gY"
      },
      "outputs": [],
      "source": [
        "model.model.save_pretrained(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHTcJ9UHYfZn"
      },
      "outputs": [],
      "source": [
        "model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCSAF6yvK5S_"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = '/content/drive/MyDrive/PROJECT MODEL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67Mf7VUdK5xv"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "log_dir = \"lightning_logs/version_0/\"  # Adjust path if needed\n",
        "event_file = [log_dir + f for f in os.listdir(log_dir) if \"events.out.tfevents\" in f][0]\n",
        "\n",
        "event_acc = EventAccumulator(event_file)\n",
        "event_acc.Reload()\n",
        "\n",
        "# Get all available log keys\n",
        "available_logs = event_acc.Tags()[\"scalars\"]\n",
        "print(\"Available logs:\", available_logs)\n"
      ],
      "metadata": {
        "id": "oht3hkKfJGOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}